---
title: "R Notebook"
output: html_notebook
---


```{r}
library(keras)
library(lime)
library(tidyquant)
library(rsample)
library(yardstick)
library(corrr)
library(recipes)
set.seed(123)
data <- read.csv("voice.csv")
```

```{r}
glimpse(data)
```


```{r}
set.seed(19)

train_test_split <- initial_split(data, prob = 0.8)
train <- training(train_test_split)
test <- testing(train_test_split)
```


```{r}
train %>% mutate(label = label %>% as.numeric()) %>% correlate() %>% focus(label) %>% fashion()
```

```{r}
rec_obj <- recipe(label ~., data = train) %>% step_scale(all_predictors(), -all_outcomes()) %>% prep(data = train)
rec_obj
```

```{r}
x_train <- bake(rec_obj, new_data = train) %>% select(-label)
x_test <- bake(rec_obj, new_data = test) %>% select(-label)
y_train_vec <- ifelse(pull(train, label) == "female",1, 0)
y_test_vec <- ifelse(pull(test, label) == "female",1 ,0)
```

```{r}

model_keras <- keras_model_sequential()
model_keras %>%
  layer_dense(
    units = 72,
    kernel_initializer = "uniform",
    activation = "relu",
    input_shape = ncol(x_train)
  ) %>%
 layer_dropout(rate = 0.3) %>%
  
  layer_dense (
    units = 72,
    kernel_initializer = "uniform",
    activation = "relu"
  ) %>%
  layer_dropout(rate = 0.3) %>%
layer_dense(
  units = 1,
  kernel_initializer = "uniform",
  activation = "sigmoid"
) %>%
  compile (
    optimizer = 'adam',
    loss = "binary_crossentropy",
    metrics = c("accuracy")
  )
model_keras
```


```{r}
history <- fit(
  object           = model_keras, 
  x                = as.matrix(x_train), 
  y                = y_train_vec,
  batch_size       = 100, 
  epochs           = 80,
  validation_split = 0.20
)

```

```{r}
print(history)
plot(history)
```

```{r}
yhat_keras_class_vec <- predict_classes(object = model_keras, x = as.matrix(x_test)) %>%
    as.vector()

yhat_keras_prob_vec  <- predict_proba(object = model_keras, x = as.matrix(x_test)) %>%
    as.vector()
options(yardstick.event_first = FALSE)
estimates_keras_tbl <- tibble(
  truth      = as.factor(y_test_vec) %>% fct_recode(yes = "1", no = "0"),
  estimate   = as.factor(yhat_keras_class_vec) %>% fct_recode(yes = "1", no = "0"),
  class_prob = yhat_keras_prob_vec
)
estimates_keras_tbl
truth = as.factor(y_test_vec) %>% fct_recode(yes = "1", no = "0")
```


```{r}

save_model_hdf5(model_keras, "deeplearning_2.h5")

```

```{r}
estimates_keras_tbl %>% conf_mat(truth, estimate)
estimates_keras_tbl %>% roc_auc(truth, class_prob)
estimates_keras_tbl %>% yardstick::precision(truth, estimate)
estimates_keras_tbl %>% yardstick::recall(truth, estimate)
estimates_keras_tbl %>% metrics(truth, estimate)

label_truth <- as.data.frame(ifelse(encodingSoundTest_1 == 2, "yes", "no"))

keras_roc_curve <- estimates_keras_tbl %>% roc_curve(truth, class_prob)
autoplot(keras_roc_curve)


```

```{r}
## mlp
auc_mlp <- estimates_keras_tbl %>% roc_auc(truth, class_prob)
keras_roc_curve <- estimates_keras_tbl %>% roc_curve(truth, class_prob)

```

```{r}
### glm
predictedGender_glm <- as.data.frame(predict(genderRFE, newdata = soundTest_1, type = "prob")$female)
glm_pred_df <- cbind(label_truth, predictedGender_glm)
colnames(glm_pred_df) <- c("truth", "class_prob")
glm_pred_df <- glm_pred_df %>% mutate(truth = truth %>% as.factor())

auc_glm <- roc_auc(glm_pred_df, truth, class_prob)

glm_roc_curve <- roc_curve(glm_pred_df, truth, class_prob)
autoplot(glm_roc_curve)

```

```{r}
## rf
predictedGender_rf <- as.data.frame(predict(genderForest_2, newdata = soundTest_1, type = "prob")$female)
rf_pred_df <- cbind(label_truth, predictedGender_rf)
colnames(rf_pred_df) <- c("truth", "class_prob")
rf_pred_df <- rf_pred_df %>% mutate(truth = truth %>% as.factor())

auc_rf <- roc_auc(rf_pred_df, truth, class_prob)

rf_roc_curve <- roc_curve(rf_pred_df, truth, class_prob)
autoplot(rf_roc_curve)
```

```{r}
## xgb
predictedGender_xgb <- as.data.frame(predict(genderXgboost_2, newdata = soundTest_1, type = "prob")$female)
xgb_pred_df <- cbind(label_truth, predictedGender_xgb)
colnames(xgb_pred_df) <- c("truth", "class_prob")
xgb_pred_df <- xgb_pred_df %>% mutate(truth = truth %>% as.factor())

auc_xgb <- roc_auc(xgb_pred_df, truth, class_prob)

xgb_roc_curve <- roc_curve(xgb_pred_df, truth, class_prob)
autoplot(xgb_roc_curve)
```


```{r}
## gbm
predictedGender_gbm <- as.data.frame(predict(genderGBM_2, newdata = soundTest_1, type = "prob")$female)
gbm_pred_df <- cbind(label_truth, predictedGender_gbm)
colnames(gbm_pred_df) <- c("truth", "class_prob")
gbm_pred_df <- gbm_pred_df %>% mutate(truth = truth %>% as.factor())

auc_gbm <- roc_auc(gbm_pred_df, truth, class_prob)

gbm_roc_curve <- roc_curve(gbm_pred_df, truth, class_prob)
autoplot(gbm_roc_curve) 
```

```{r}

plot1 <- autoplot(glm_roc_curve) + ggtitle("GLM") + annotation_custom(grid.text(paste0("AUC: ", round(auc_glm$.estimate, digits = 3)), x=0.8,  y=0.05, gp=gpar(fontsize=8)))

plot2 <- autoplot(rf_roc_curve) + ggtitle("Random Forest") + annotation_custom(grid.text(paste0("AUC: ", round(auc_rf$.estimate, digits = 3)), x=0.8,  y=0.05, gp=gpar(fontsize=8)))

plot3 <- autoplot(xgb_roc_curve) + ggtitle("XgBoost") + annotation_custom(grid.text(paste0("AUC: ", round(auc_xgb$.estimate, digits = 3)), x=0.8,  y=0.05, gp=gpar(fontsize=8)))

plot4 <- autoplot(gbm_roc_curve) + ggtitle("GBM") + annotation_custom(grid.text(paste0("AUC: ", round(auc_gbm$.estimate, digits = 3)), x=0.8,  y=0.05, gp=gpar(fontsize=8)))

plot5 <- autoplot(keras_roc_curve) + ggtitle("MLP") + annotation_custom(grid.text(paste0("AUC: ", round(auc_mlp$.estimate, digits = 3)), x=0.8,  y=0.05, gp=gpar(fontsize=8)))

grid.arrange(plot1, plot2, plot3, plot4, plot5, ncol=3)
```

```{r}
class(model_keras)
```


```{r}
model_type.keras.models.Sequential <- function(x, ...) {
  "classification"
}

predict_model.keras.models.Sequential <- function(x, newdata, type, ...) {
  pred <- predict_proba(object = x, x = as.matrix(newdata))
  data.frame(Yes = pred, No = 1 - pred)
}
```

```{r}
predict_model(x = model_keras, newdata = x_test, type = 'raw') %>%
  tibble::as_tibble()
```

```{r}
explainer <- lime::lime(
  x              = x_train, 
  model          = model_keras, 
  bin_continuous = FALSE
)
```

```{r}

explanation <- lime::explain(
  x_test[1:100,], 
  explainer    = explainer, 
  n_labels     = 1, 
  n_features   = 5,
  kernel_width = 0.5
)

```


```{r}
plot_explanations(explanation) +
    labs(title = "LIME Feature Importance Heatmap",
         subtitle = "Hold Out (Test) Set, First 100 Cases Shown")
```

```{r}

corrr_analysis <- x_train %>%
  mutate(label = y_train_vec) %>%
  correlate() %>%
  focus(label) %>%
  rename(feature = rowname) %>%
  arrange(abs(label)) %>%
  mutate(feature = as_factor(feature)) 
corrr_analysis
```

```{r}

corrr_analysis %>%
  ggplot(aes(x = label, y = fct_reorder(feature, desc(label)))) +
  geom_point() +
  # Positive Correlations - Contribute to male
  geom_segment(aes(xend = 0, yend = feature), 
               color = palette_light()[[2]], 
               data = corrr_analysis %>% filter(label > 0)) +
  geom_point(color = palette_light()[[2]], 
             data = corrr_analysis %>% filter(label > 0)) +
  # Negative Correlations - contribute to female
  geom_segment(aes(xend = 0, yend = feature), 
               color = palette_light()[[1]], 
               data = corrr_analysis %>% filter(label < 0)) +
  geom_point(color = palette_light()[[1]], 
             data = corrr_analysis %>% filter(label < 0)) +
  # Vertical lines
  geom_vline(xintercept = 0, color = palette_light()[[5]], size = 1, linetype = 2) +
  geom_vline(xintercept = -0.25, color = palette_light()[[5]], size = 1, linetype = 2) +
  geom_vline(xintercept = 0.25, color = palette_light()[[5]], size = 1, linetype = 2) +
  # Aesthetics
  theme_tq() +
  labs(title = "Gender Correlation Analysis",
       subtitle = paste("Positive Correlations (contribute to female),",
                        "Negative Correlations (contribute to male)"),
       y = "Feature Importance")

```

```{r}

model_to_transfer <- keras_model_sequential()
model_to_transfer %>%
  layer_dense(
    units = 16,
    kernel_initializer = "uniform",
    activation = "relu",
    input_shape = ncol(x_train)
  ) %>%
 layer_dropout(rate = 0.3) %>%
  
  layer_dense (
    units = 16,
    kernel_initializer = "uniform",
    activation = "relu"
  ) %>%
  compile (
    optimizer = 'adam',
    loss = "binary_crossentropy",
    metrics = c("accuracy")
  )
model_to_transfer

```

```{r}
transfer_model <- fit(
  object           = model_to_transfer, 
  x                = as.matrix(x_train), 
  y                = y_train_vec,
  batch_size       = 50, 
  epochs           = 100,
  validation_split = 0.20
)

```
